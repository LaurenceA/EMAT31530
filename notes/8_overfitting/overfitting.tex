\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools}
\usepackage{parskip}
\usepackage{color}
\usepackage{booktabs}
\usepackage{hyperref}

\newtheorem{exercise}{Exercise}
\newtheorem{answer}{Answer}

\newcommand{\dd}[2][]{\frac{\partial #1}{\partial #2}}
\newcommand{\dt}[2][]{\frac{d #1}{d #2}}
\newcommand{\dL}{\dt[{\L}]}
\newcommand{\dLi}{\dt[{\Li}]}
\newcommand{\dLmb}{\dt[{\Lmb}]}
\newcommand{\dLfb}{\dt[{\Lfb}]}
\newcommand{\yh}{\hat{y}}

\newcommand{\bracket}[3]{\left#1 #3 \right#2}
\newcommand{\sqb}{\bracket{[}{]}}
\newcommand{\ab}{\bracket{\langle}{\rangle}}
\renewcommand{\b}{\bracket{(}{)}}
\newcommand{\abs}{\bracket{\lvert}{\rvert}}

\newcommand{\0}{\mathbf{0}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\h}{\mathbf{h}}
\renewcommand{\d}{\mathbf{d}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\mom}{\mathbf{v}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\gfb}{\g_\text{fb}}
\newcommand{\gmb}{\g_\text{mb}}
\newcommand{\gmbt}{\g_{\text{mb}; t}}
\newcommand{\gsmbt}{g_{\text{mb}; t}}
\newcommand{\gsfb}{g_\text{fb}}
\newcommand{\gsmb}{g_\text{mb}}
\newcommand{\gsd}{\g_\text{sd}}
\newcommand{\vh}{\hat{v}}
\newcommand{\mh}{\hat{m}}
\newcommand{\gh}{\mathbf{\hat{g}}}
\newcommand{\gb}{\mathbf{\ab{g}}}
\newcommand{\gssqb}{\ab{g^2}}
\newcommand{\gsb}{\ab{g}}
\newcommand{\bv}{\mathbf{b}}
\newcommand{\ov}{\mathbf{o}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\I}{\mathbf{I}}
\renewcommand{\P}{\operatorname{P}\b}

\newcommand{\w}{\mathbf{w}}
\newcommand{\wo}{\w^*}

\renewcommand{\L}{\mathcal{L}}
\newcommand{\Lreg}{\L_\text{reg}}
\newcommand{\Lunreg}{\L_\text{unreg}}
\newcommand{\dLreg}{\dt[\Lreg]}
\newcommand{\dLunreg}{\dt[\Lunreg]}
\newcommand{\greg}{g_\text{reg}}
\newcommand{\gunreg}{g_\text{unreg}}
\newcommand{\Li}{\L_i}
\newcommand{\Lmb}{\L_\text{mb}}
\newcommand{\Lfb}{\L_\text{fb}}
\newcommand{\E}{\operatorname{E}\sqb}
\newcommand{\Var}{\operatorname{Var}\sqb}

\newcommand{\logits}{\ell}
\newcommand{\vlogits}{\boldsymbol{\logits}}
\newcommand{\softmax}{\operatorname{softmax}}
\newcommand{\linear}{\operatorname{linear}}
\newcommand{\relu}{\operatorname{relu}}
\newcommand{\sqerr}{\operatorname{sqerr}}
\newcommand{\dropout}{\operatorname{dropout}}

\newcommand{\func}{\operatorname{func}}
\newcommand{\funcback}{\operatorname{func{.}backward}}
\newcommand{\funcjac}{\operatorname{func{.}jacobian}}
\newcommand{\inputs}{\operatorname{inputs}}
\newcommand{\outputs}{\operatorname{outputs}}

\newcommand{\linearback}{\operatorname{linear{.}backward}}
\newcommand{\reluback}{\operatorname{relu{.}backward}}
\newcommand{\sqerrback}{\operatorname{sqerr{.}backward}}

\newcommand{\linearjac}{\operatorname{linear{.}jacobian}}
\newcommand{\relujac}{\operatorname{relu{.}jacobian}}

\newcommand{\iinmb}{{i \text{ in mb}}}
\newcommand{\mbsize}{M}
\newcommand{\mbavg}{\tfrac{1}{\mbsize} \sum_{\mathclap{\iinmb}}}

\newcommand{\iinfb}{{i \text{ in fb}}}
\newcommand{\fbsize}{N}
\newcommand{\fbavg}{\tfrac{1}{\fbsize} \sum_{i=1}^{\fbsize}}
\newcommand{\lrtrad}{\eta_\text{trad}}
\newcommand{\lravg}{\eta_\text{avg}}

\title{EMAT31530: Overfitting, and other topics in NN training}
\author{Laurence Aitchison}
\date{}

\begin{document}

\maketitle

I advise that you take a look at the iPython notebook for this week first, as it discusses overfitting in the simpler setting of linear models.  
Here, we take the next step, by discussing overfitting, and strategies to mitigate overfitting, in the more complex setting of neural networks.

\section{Overfitting is not catastrophic in NNs}

In the 1990's, neural networks were largely ignored: only a few people like Yann LeCun were still working on them.
One of the reasons was the following reasoning:
\begin{itemize}
  \item Neural networks have loads (and loads) of parameters.
  \item Models with loads of parameters tend to overfit.
  \item So neural networks should overfit really badly.
\end{itemize}
That led many people to expect that overfitting in NNs would be catastropic.
In actuality, it turns out that overfitting in NNs isn't catastropic at all: if you ignore overfitting while training your neural net, you should get a pretty reasonable fit.
But overfitting is still an issue, and there are a large number of approaches to mitigating overfitting in neural networks.
That's what we'll look at this week.


\newpage
\section{Weight decay}
\includegraphics[width=\textwidth]{weight_decay.pdf}
Perhaps the most standard approach to mitigating overfitting is weight-decay.
In weight decay, we add a term to the loss, penalising large weights,
\begin{align}
  \label{eq:weight_decay_loss}
  \Lreg = \Lunreg + \tfrac{\lambda}{2} \sum_i w_i^2.
\end{align}
Here, $\Lunreg$ is the original unregularised loss (e.g.\ just the cross-entropy), and $\Lreg$ is the regularised loss.
(To keep consistency with the \href{https://pytorch.org/docs/stable/generated/torch.optim.SGD.html}{PyTorch docs}, we assume that $\Lreg$ and $\Lunreg$ are the loss for a single minibatch.)
Importantly, Eq.~\eqref{eq:weight_decay_loss} does not yet look like weight ``decay''.  

To understand why ``weight decay'' is called ``weight decay'', consider the gradients for a single parameter, $w$,
\begin{align}
  \gunreg &= \dLunreg{w}\\
  \greg &= \dLreg{w} = \dLunreg{w} + \lambda w
\end{align}
Thus, the SGD update becomes,
\begin{align}
  \Delta w &= - \eta \greg = \underbrace{- \eta \dLunreg{w}}_\text{original update} \underbrace{- \eta \lambda w}_\text{decay term}
\end{align}
Thus, the update now contains a decay term, $-\eta \lambda w$, that pushes $w$ towards zero.

\newpage
\section{``Decoupled'' weight decay, or AdamW}
\includegraphics[width=\textwidth]{decoupled_weight_decay.pdf}

Unfortunately, the simple connection between weight-decay and a modified loss breaks down when we consider RMSProp/Adam.
Specifically, consider just using the modified loss (Eq.~\ref{eq:weight_decay_loss}) with RMSProp, (we use RMSProp rather than Adam in our examples as it simplifies things slightly while keeping the effect we're interested in),
\begin{align}
  \label{eq:adamL2}
  \Delta w &= -\eta \frac{\greg}{\sqrt{\ab{\greg^2}}} = -\eta \frac{\gunreg + \lambda w}{\sqrt{\ab{\greg^2}}}\\
  \intertext{And splitting up terms,}
  \Delta w &= \underbrace{-\eta \frac{\gunreg}{\sqrt{\ab{\greg^2}}}}_\text{original update} \quad\underbrace{- \eta \lambda \frac{w_i}{\sqrt{\ab{\greg^2}}}}_\text{Adam weight decay}
\end{align}
This equation is a bit odd, as the updates don't really seem to really take on a simple ``weight decay'' form.
Instead, the weight decay is also normalized by the exponential moving average gradient-squared.
Unfortunately for historical reasons this form is termed the ``standard'' version of weight decay, so the terms ``weight decay'', ``standard weight decay'', ``Adam with weight decay'' all refer to updates like this.

That raises a question: what if we ``decoupled'' the weight decay from the normalizer, and just used,
\begin{align}
  \label{eq:adamw}
  \Delta w &= \underbrace{-\eta \frac{\gunreg}{\sqrt{\ab{\gunreg^2}}}}_\text{original update} \quad \underbrace{- \eta \lambda w_i}_\text{``decoupled'' weight decay}
\end{align}
This is ``decoupled'' weight decay.  And Adam with decoupled weight decay is known as AdamW.

What consequences does this have?
Well, I'm not sure anyone really knows.
But at least, it means that the AdamW/decoupled weight decay updates cannot be written in terms of minimizing a loss.
That makes these updates incredibly weird: almost all of deep learning is derived from optimizing a loss function.
I'm not sure the field has really thought-through the consequences of this point ... but it is really weird.


%Second, we can understand the differences between the Adam and AdamW updates by considering a ``toy'' setting with a single parameter, with a gradient, $g$, that's constant across minibatches and constant across a large range of parameters.
%We can now solve for the steady-state value of $w$.
%For Adam, as everything is framed in terms of an objective, we can find the steady-state by finding the location at which the regularised gradients are zero,
%\begin{align}
%  0 &= \greg = \gunreg + \lambda w\\
%  w &= -\frac{\gunreg}{\lambda}.
%\end{align}
%So stronger gradients imply bigger weights.
%In contrast, AdamW can't be framed in terms of an objective, so we need to work with the full updates,
%\begin{align}
%  \Delta w &= -\eta \frac{\ab{\gunreg}}{\sqrt{\ab{\gunreg^2}}} - \eta \lambda w_i.
%\end{align}
%Following the analysis of RMSProp last week, we're going to take,
%\begin{align}
%  \frac{\ab{\gunreg}}{\sqrt{\ab{\gunreg^2}}} \approx \text{sign}(\gunreg),
%\end{align}
%Thus, at steady-state,
%\begin{align}
%  0 &\approx -\eta \text{sign}(\gunreg) - \eta \lambda w_i,\\
%  w_i &\approx \frac{\text{sign}(\gunreg)}{\lambda}.
%\end{align}
%So the steady-state weights for AdamW point in the same direction as the gradients, but they don't scale with the gradients!



\newpage
\section{Dropout}
\includegraphics[width=\textwidth]{dropout.pdf}
Dropout is perhaps the most well-known of a class of methods that add some kind of noise to the training process.
The basic idea in dropout is to ``turn off'' features with some probability (typically 0.5), and to scale up the other features to compenstate.
This seems to regularise neural network training (there are some more specific ideas for what its doing, but I'm not sure I believe any of them).

More formally, dropout samples a binary mask. 
The probability $p$ is the probability of dropping out a feature (i.e.\ the probability of the mask being $0$!).
For instance, if a hidden layer has 14 features, this binary mask might look like:
\begin{align}
  \mathbf{m} &= (0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0).
\end{align}
The dropout operation then:
\begin{align}
  \dropout_i^\text{train}(\h) = \tfrac{1}{1-p} m_i h_i
\end{align}
The mask drops out features, while the $\tfrac{1}{1-p}$ factor multiplicatively increases the other factors to compensate.

Importantly, this only happens at \textit{train} time.  At test time we usually want a determinstic output.  And to get a deterministic output the dropout layers stop doing anything:
\begin{align}
  \dropout^\text{test}(\h) = \h.
\end{align}
Now, we can see where the $\tfrac{1}{1-p}$ factor comes from.
Specifically, we want the expected output of dropout at train-time to be the same as the output of dropout at test-time.
\begin{align}
  \E{\dropout_i^\text{train}(\h)} = \tfrac{1}{1-p} \E{m_i} h_i = \tfrac{1-p}{1-p} h_i = h_i = \dropout_i^\text{test}(\h).
\end{align}
Remember that $p$ is the probability of dropping out a feature (not of keeping the feature).  So $\E{m_i} = 1-p$,

A network with dropout might look like:
\begin{subequations}
\label{eq:forward}
\begin{align}
  \a_1 &= \linear\b{\x, \W_1, \bv_1}\\
  \h_1 &= \relu\b{\a_1}\\
  \d_1 &= \dropout\b{\h_1}\\
  \a_2 &= \linear\b{\d_1, \W_2, \bv_2}\\
  \h_2 &= \relu\b{\a_2}\\
  \d_2 &= \dropout\b{\h_2}\\
  \f &= \linear\b{\d_2, \W_3, \bv_3}\\
  \label{eq:L}
  \L &= \sqerr\b{\f, \y}.
\end{align}
\end{subequations}

Finally, one important point to raise about dropout is that it raises the possibility of lots of irritiating bugs.
Specifically, the network now behaves differently at train and test time.
So you need to set the network in training mode, or in evaluation model.
Using \verb|torch.nn| modules you set the network into training mode using
\begin{verbatim}
  net.train()
\end{verbatim}
and you set the network into eval mode using 
\begin{verbatim}
  net.eval()
\end{verbatim}
Obviously, its super-easy to forget how to do this.  And if you do forget, it won't be obvious (it'll still work, just not quite as well as it might have done).



\newpage
\section{Adam vs SGD}

\includegraphics[width=\textwidth]{adam_vs_sgd.pdf}

Here, I ran both Adam and SGD for a long time (1000 iterations).
It seems that:
\begin{itemize}
  \item Adam is better at capturing ``fine details'' than SGD.
  \item Adam tends to overfit more than SGD.
\end{itemize}
Of course, these are really just two sides of the same coin: overfitting \textit{is} capturing the fine details (specifically, fine details in the noise).
Moreover, note that these patterns obviously depend on the ``hyperparameters'' (e.g.\ learning rate and momentum).  
But this is the general ``rule of thumb'' that does seem to hold very generally in NNs.

So which is best Adam or SGD?  Depends on whether you care about mitigating overfitting or capturing fine-details.
\begin{itemize}
  \item SGD tends to be better for ConvNets (which we'll look at next week) for image recognition, as it mitigates overfitting.
  \item Adam tends to be better for transformers (which we'll look at in a couple of weeks) and RL where routing information in complex ways is important.
\end{itemize}

\newpage
\section{Early stopping}

\includegraphics[width=\textwidth]{early_stopping.pdf}

%Here, I fitted a three-layer network with width 100 hidden layer, to 20 datapoints drawn from $\sin(x) + \text{noise}$.
%I used SGD with a learning rate of 0.01, and momentum of 0.8.
%I then plotted the prediction at different points throughout training.

Here, I trained using SGD, with a learning rate of 0.1, and momentum of 0.8.
Late on in training (darker lines), the line overfits, as it basically goes through all the datapoints. 
In contrast, earlier on in training (lighter lines), we get a smoother curve, and hence less overfitting.
So we can use ``early stopping'' as a regularisation technique!
And early stopping is great because it gives us a nice excuse to cut training short.

\newpage
\section{Minibatch noise}

\begin{center}
  \includegraphics[width=0.5\textwidth]{schedule.png}
\end{center}

Typical practice in training neural networks involves starting with a high learning rate, then decaying the learning rate in steps.
In the above plot (blue line), the learning rate is decayed by a factor of 5 in steps at epochs 60 and 120 (epochs is a measure of time through training; one epoch means we've gone over all datapoints once).
Notice that as you decay the learning rate, the training cross-entropy drops quickly, then becomes somewhat static.
What seems to be going on is that minibatch noise stops you from converging to the actual optimum.
Instead, minibatch noise causes you to bounce around the optimum.
You can reduce the effective amount of noise by reducing the learning rate (see previous section on Optimizers).
Alternatively (red line), you can reduce the effective noise by increasing the batch size, so each batch averages over more datapoints.
However, we usually reduce the learning rate rather than increasing the batch size, as we don't have the GPU memory to increase the batch size.

Interestingly, that means we can use the noise induced by SGD with small batches and high learning rates as a regulariser!
And that causes lots of confusion in deep learning: is it better to use high learning rates and small batches to get lots of minibatch noise, and hence regularise more?  
Or is it better to use lower learning rates and larger batches to get less minibatch noise and hence converge better to the optimum?
Well, like many thing in AI, it depends on lots of details of the task, dataset, architecture etc.\ and the only real way of finding out in any given setting is to experiment.

\section{Exercises}

No exercises this week!


\end{document}

